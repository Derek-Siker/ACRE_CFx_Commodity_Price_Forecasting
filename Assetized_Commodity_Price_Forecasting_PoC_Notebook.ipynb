{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "306e1f9a",
   "metadata": {},
   "source": [
    "# Commodity Price Forecasting with Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be2e60f",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9efe857",
   "metadata": {},
   "source": [
    "- #### Introduction\n",
    "\n",
    "    - In this project, a number of tree-based regressors are deployed to predict the monthly price direction and concurrent median spot price of a chosen commodity contract over a select period of time (the client will need to specify the type of contract or averaging method across contracts that should be used). \n",
    "\n",
    "\n",
    "- #### Data compatibility\n",
    "\n",
    "    - This code will work for any market data ingested via Barchart to construct the target feature as well either Barchart or Oilworld physical contract data for input features. \n",
    "\n",
    "- #### Exploratory data analysis\n",
    "\n",
    "    - Standard pandas profiling reports are provided along with basic summary statistics to inform skew and missingness in the data.\n",
    "\n",
    "\n",
    "- #### Data wrangling and pre-processing\n",
    "\n",
    "    - The current approach converts time series data into a usable format for a supervised machine learning problem and applies the following standard transformations:\n",
    "       - Handling missing data (represented as NaNs or zeros) using forward-backfill mean imputation. This method can also be substituted for general mean imputation if required\n",
    "       - Removal of data skew using log transformation\n",
    "       - Normalization with sklearn MinMax scaling\n",
    "\n",
    "\n",
    "- #### Feature engineering\n",
    "\n",
    "    - Automated feature engineering is included in the template of this code. Examples are also provided for information that may be helpful to engineer into additional input features (consult with your CST, the client, and potentially use ENS Navigator to set up expert interviews for identifying targets specific to your use case). \n",
    "\n",
    "- #### Feature transformation\n",
    "\n",
    "    - The option is provided to apply dimensionality reduction using the PCA algorithm from sklearn. The number of dimensions to be used should be specified by the user.\n",
    "\n",
    "\n",
    "- #### Feature selection \n",
    "\n",
    "    - Feature selection defaults to the Boruta method and reverts to F-test selection if no features are initially chosen. Setting a lower iteration rate may also help to reduce run-time and expand the selection, but may also reduce model accuracy.\n",
    "\n",
    "- #### Model selection and evaluation approach\n",
    "\n",
    "    - A tree-based model is required to run the code (any type of ensemble or boosted-tree model is compatible but may require additional updates to params grid and confidence interval generator). Currently the model will default to sklearn's RandomForestRegressor.\n",
    "\n",
    "    - Backtesting is applied using a time series GridSearch expanding window cross-validation approach - the training period must be specified by the user. The model will train up to each point in an expanding window and then make an out-of-sample prediction. These out-of-sample predictions are then collected into a final dataframe along with confidence intervals (currently generated by forestci). This dataframe is then further transformed to convert predictions from % difference between current and future median contract price into spot price predictions.\n",
    "\n",
    "    - The model is evaluation using standard regression metrics (MAE, MAPE) as well as directional accuracy (precision, recall, and F1 scores). For directional accuracy the option is also provided for both binary and multi-class confusion matrices; the client will need to provide information on the cut-off points to be used for constructing classes. \n",
    "\n",
    "\n",
    "- #### Feature importance\n",
    "\n",
    "    - A gini-importance score is used to rank and plot feature importances (using sklearn package)\n",
    "\n",
    "\n",
    "- #### Final output and visualization\n",
    "\n",
    "    - The final predictions, confidence intervals, and information related to actual price as well as % change in price are saved in both .csv and .xlsx format. \n",
    "\n",
    "    - Additionally, an ouptut file is provided in both .csv and .xlsx format for populating an MVP PowerBI Dashboard. In order to fully utilize the dashboard, output may also be required from the ACRE Commodity FX hedging optimization support tool.\n",
    "\n",
    "    - A version of the dashboard is also in development that will only require price forecasting output from this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0ccf59",
   "metadata": {},
   "source": [
    "## How to run this notebook\n",
    "\n",
    "- All code boxes requring variable or filename entry will be marked with three stars *** at the top and three dots ... to represent areas where more than one variable may be specified. Please don't forget to remove these before running the code.\n",
    "- Before running the code, make sure to first run package import and all helper functions at the bottom of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1e455d",
   "metadata": {},
   "source": [
    "### Import packages and input display settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f326666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages to process data + run/evaluate the model \n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from pandas_profiling import ProfileReport\n",
    "import forestci as fci\n",
    "from boruta import BorutaPy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, TimeSeriesSplit\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import recall_score, make_scorer, accuracy_score, fbeta_score, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.exceptions import NotFittedError, DataConversionWarning\n",
    "from typing import Tuple, List, Dict\n",
    "from talib import RSI, BBANDS, MACD\n",
    "import copy\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd14ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "# Ignore irrelevant warning messages\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "# Set figure size for seaborn plots\n",
    "sns.set(rc={'figure.figsize':(11, 4)})\n",
    "sns.set_style('whitegrid')\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b850159",
   "metadata": {},
   "source": [
    "## Exploring the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b0f261",
   "metadata": {},
   "source": [
    "### Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac8910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "# Load Barchart market data for each set of commodities contracts to be used (these may be used for either input features or target)\n",
    "# This data should be accesible in csv format through McKinsey ACRE license with Barchart and must be uploaded into your Jupyter notebook environment in order to run the code\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    _INSERT VARIABLE NAME HERE_ = pd.read_csv(\"_INSERT FILE NAME HERE_.csv\")\n",
    "    _INSERT VARIABLE NAME HERE_.name = \"_INSERT VARIABLE NAME HERE_\"\n",
    "    \n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "    ...\n",
    "    ...\n",
    "                                                  \n",
    "###Examples:\n",
    "#     Soybean_Oil_US = pd.read_csv(\"Soybean_oil_US.csv\")\n",
    "#     Soybean_Oil_US.name = \"Soybean_Oil_US\"\n",
    "\n",
    "#     CPO_USD_MDEX_Malaysia = pd.read_csv(\"CPO_USD_MDEX_Malaysia.csv\")\n",
    "#     CPO_USD_MDEX_Malaysia.name = \"CPO_USD_MDEX_Malaysia\"\n",
    "\n",
    "#     Indian_Rupee_USD = pd.read_csv(\"Indian_Rupee_USD.csv\")\n",
    "#     Indian_Rupee_USD.name = \"Indian_Rupee_USD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0853fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "# Load Oilworld physical contract data\n",
    "# This data should be accesible in xlsx format through McKinsey ACRE license with Oilworld and must be uploaded into your Jupyter notebook environment in order to run the code\n",
    "\n",
    "Oilworld_data = pd.read_excel(\"_INSERT FILE NAME HERE_.xlsx\")\n",
    "Oilworld_data['MONTHLY PRICES'] = Oilworld_data['MONTHLY PRICES'].apply(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b6b83a",
   "metadata": {},
   "source": [
    "### Summary statistics and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a36420",
   "metadata": {},
   "source": [
    "#### Barchart data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34bffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "#run profile report for any of the given data - run separately for each dataset from Barchart\n",
    "profile = ProfileReport(_INSERT VARIABLE NAME HERE_, title=\"Barchart Pandas Profiling Report\", explorative=True)\n",
    "profile.to_widgets()\n",
    "\n",
    "###Example\n",
    "# profile = ProfileReport(CPO_USD_MDEX_Malaysia, title=\"Barchart Pandas Profiling Report\", explorative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde78e94",
   "metadata": {},
   "source": [
    "#### Oilworld data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f7d39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(Oilworld_data, title=\"Oilword Pandas Profiling Report\", explorative=True)\n",
    "profile.to_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76779bc9",
   "metadata": {},
   "source": [
    "#### Target Feature Exploration + Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1a2e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "target_data = _INSERT TARGET VARIABLE NAME HERE_.copy()\n",
    "target_data = target_data.set_index('timestamp')\n",
    "\n",
    "###Example\n",
    "# target_data = CPO_USD_MDEX_Malaysia.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc01d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the contract closest to expiration each month\n",
    "target_data = find_closest_to_spot(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef8cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###TARGET DEFINITION\n",
    "\n",
    "###OPTION 1\n",
    "#% change between the current price and 1-3 month median and 4-6 month median for the next 6 months of selected futures contract closest to spot price\n",
    "\n",
    "# Calculate median 1-3 \n",
    "target_data['Median_1_3'] = target_data['settle'].rolling(window=3).median()\n",
    "target_data['Median_1_3'] = target_data['Median_1_3'].shift(-2)\n",
    "\n",
    "# Calculate median 4-6 \n",
    "target_data['Median_4_6'] = target_data['settle'].shift(-3).rolling(window=3).median()\n",
    "\n",
    "# Calculate % change column between current price and 1-3 month median, then do another column for 4-6 month\n",
    "target_data['%_diff_price_current_vs_Median_1_3'] = (target_data['Median_1_3'] - target_data['settle']) / target_data['settle']\n",
    "target_data['%_diff_price_current_vs_Median_4_6'] = (target_data['Median_4_6'] - target_data['settle']) / target_data['settle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a968cd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use settle price to determine summary statistics\n",
    "prices = target_data.settle\n",
    "\n",
    "# Minimum price of the data\n",
    "minimum_price = np.amin(prices)\n",
    "\n",
    "# Maximum price of the data\n",
    "maximum_price = np.amax(prices)\n",
    "\n",
    "# Mean price of the data\n",
    "mean_price = np.mean(prices)\n",
    "\n",
    "# Median price of the data\n",
    "median_price = np.median(prices)\n",
    "\n",
    "# Standard deviation of prices of the data\n",
    "std_price = np.std(prices)\n",
    "\n",
    "\n",
    "# Show the calculated statistics\n",
    "print(\"Statistics for CPO settle price closest to spot:\\n\")\n",
    "print(\"Minimum price: ${:,.2f}\".format(minimum_price))\n",
    "print(\"Maximum price: ${:,.2f}\".format(maximum_price))\n",
    "print(\"Mean price: ${:,.2f}\".format(mean_price))\n",
    "print(\"Median price ${:,.2f}\".format(median_price))\n",
    "print(\"Standard deviation of prices: ${:,.2f}\".format(std_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848a3f57",
   "metadata": {},
   "source": [
    "#### Price distribution analysis\n",
    "Visualize price distribution, derive percentiles and then use this to generate target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf066a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of commodity futures contract prices closest to expiration for across entire time period\n",
    "hist = target_data.settle.hist(density=True)\n",
    "plt.title(\"Histogram of Commodity Price Distribution\")\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "# Fit a normal distribution to the data:\n",
    "# mean and standard deviation\n",
    "mu, std = norm.fit(target_data.settle) \n",
    "  \n",
    "# Plot the PDF.\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "  \n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d90709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Density plot of commodity futures contract price distribution by month\n",
    "kde = target_data.settle.plot(kind='kde')\n",
    "plt.title(\"Density plot of Commodity Price Distribution by Month\")\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Probability Density\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e70ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Density plot of commodity futures contract price distribution Z-score by month\n",
    "target_data['Zscore'] = target_data.groupby('month').settle.apply(lambda x: x.div(x.mean()))\n",
    "target_data.groupby('month').Zscore.plot.kde()\n",
    "plt.title(\"Density plot of Z-score of Commodity Price Distribution\")\n",
    "plt.xlabel(\"Z Score\")\n",
    "plt.ylabel(\"Probability Density\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96745f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    # Percentile plot of commodity price distribution over total time period\n",
    "    g = target_data\n",
    "    i = g['settle'].quantile([0.05, 0.25, 0.5, 0.9])\n",
    "    j = g['settle'].agg(['min', 'max'])\n",
    "    pd.concat([i, j], 1)\n",
    "    i.T.plot(subplots=True)\n",
    "    plt.title(\"Percentile plot of Commodity Price Distribution\")\n",
    "    plt.xlabel(\"Percentile\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.show()\n",
    "\n",
    "    # Percentile plot of commodity price distribution Z-score over total time period\n",
    "    i = g['Zscore'].quantile([0.05, 0.25, 0.5, 0.9])\n",
    "    j = g['Zscore'].agg(['min', 'max'])\n",
    "    pd.concat([i, j], 1)\n",
    "    i.T.plot(subplots=True)\n",
    "    plt.title(\"Percentile plot of Z-Score of Commodity Price Distribution\")\n",
    "    plt.xlabel(\"Percentile\")\n",
    "    plt.ylabel(\"Z-Score\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb63127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset actual price and median 1-3, 4-6 in order to convert % back to price and evaluate MAPE after running the model\n",
    "target_data_select = target_data[['price_date', 'month', 'year', 'settle', 'Median_1_3', 'Median_4_6']]\n",
    "target_data_select['price_date'] = target_data_select.price_date - pd.offsets.MonthBegin(1)\n",
    "target_data_select = target_data_select.set_index('price_date')\n",
    "target_data_select.index = pd.DatetimeIndex(target_data_select.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39dfb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset target_data to only include target features\n",
    "target_data = target_data.drop(columns=['price_date', 'settle', 'Zscore'])\n",
    "target_data['day'] = 1\n",
    "target_data['price_date'] = pd.to_datetime(target_data[['year', 'month', 'day']])\n",
    "target_data = target_data.drop(columns='day')\n",
    "target_data = target_data.set_index('price_date')                                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f42973",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0d050f",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad5d80a",
   "metadata": {},
   "source": [
    "#### Barchart alternative plant-based commodities, other related commodities, and FX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a7f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "# Run helper function on each commodity dataframe to select only closest contract to spot price per month \n",
    "# Please refer to  helper function method for 'find_closest_to_spot' for other options if the client requests a different approach)\n",
    "\n",
    "_INSERT VARIABLE NAME HERE_ = find_closest_to_spot(_INSERT VARIABLE NAME HERE_)\n",
    "...\n",
    "...\n",
    "\n",
    "\n",
    "###Examples:\n",
    "# preprocessed_Soybean_Oil_US = find_closest_to_spot(Soybean_Oil_US)\n",
    "# preprocessed_CPO_USD_MDEX_Malaysia = find_closest_to_spot(CPO_USD_MDEX_Malaysia) \n",
    "# preprocessed_Ethanol_US = find_closest_to_spot(Ethanol_US)\n",
    "# preprocessed_Indian_Rupee_USD = find_closest_to_spot(Indian_Rupee_USD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a419e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "#SET DATE CUTOFF - Run pre-processing on each commodity dataframe to select only closest contract to spot price per month\n",
    "\n",
    "_INSERT VARIABLE NAME HERE_ = preprocess_commodity_date(_INSERT VARIABLE NAME HERE_, _INSERT DATE HERE_)\n",
    "...\n",
    "...\n",
    "\n",
    "###Examples:\n",
    "# preprocessed_Soybean_Oil_US = preprocess_commodity_date(preprocessed_Soybean_Oil_US, '2010-01-01')\n",
    "# preprocessed_CPO_USD_MDEX_Malaysia = preprocess_commodity_date(preprocessed_CPO_USD_MDEX_Malaysia, '2010-01-01') \n",
    "# preprocessed_Ethanol_US = preprocess_commodity_date(preprocessed_Ethanol_US, '2010-01-01')\n",
    "# preprocessed_Indian_Rupee_USD = preprocess_commodity_date(preprocessed_Indian_Rupee_USD, '2010-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b554211",
   "metadata": {},
   "source": [
    "Data for some contracts is not always present on a monthly basis - however it will consistently be missing in the same months across all years for any given contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c08d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    _INSERT VARIABLE NAME HERE_ = crop_barchart_data(_INSERT VARIABLE NAME HERE_)\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "###Examples\n",
    "# preprocessed_Soybean_Oil_US = crop_barchart_data(preprocessed_Soybean_Oil_US)\n",
    "# preprocessed_CPO_USD_MDEX_Malaysia = crop_barchart_data(preprocessed_CPO_USD_MDEX_Malaysia)\n",
    "# preprocessed_Ethanol_US = crop_barchart_data(preprocessed_Ethanol_US)\n",
    "# preprocessed_Indian_Rupee_USD = crop_barchart_data(preprocessed_Indian_Rupee_USD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c8ec72",
   "metadata": {},
   "source": [
    "#### OIlworld alternative plant-based commodities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e1bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    Oilworld_data['month'] = [x.split('.')[0] for x in Oilworld_data['MONTHLY PRICES']]\n",
    "    Oilworld_data['month'] = Oilworld_data['month'].astype(int)\n",
    "\n",
    "    Oilworld_data['year'] = [x.split('.')[1] for x in Oilworld_data['MONTHLY PRICES']]\n",
    "    Oilworld_data['year'] = Oilworld_data['year'].astype(int)\n",
    "\n",
    "    Oilworld_data['day'] = 1\n",
    "    Oilworld_data['date'] = pd.to_datetime(Oilworld_data[['year', 'month', 'day']])\n",
    "\n",
    "    Oilworld_data = Oilworld_data.set_index('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cc4627",
   "metadata": {},
   "source": [
    "### Renaming data columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cd4997",
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "# rename price col for each barchart commodity dataframe\n",
    "_INSERT VARIABLE NAME HERE_ = _INSERT VARIABLE NAME HERE_.rename(columns={'settle': '---INSERT VARIABLE NAME HERE---'})\n",
    "...\n",
    "...\n",
    "\n",
    "###Examples\n",
    "# preprocessed_Soybean_Oil_US = preprocessed_Soybean_Oil_US.rename(columns={'settle': 'Soybean oil futures contract, USD, closest to spot price for year-month'})\n",
    "# preprocessed_CPO_USD_MDEX_Malaysia = preprocessed_CPO_USD_MDEX_Malaysia.rename(columns={'settle': 'Crude palm oil futures contract, USD (MDEX), closest to spot price for year-month'})\n",
    "# preprocessed_Ethanol_US = preprocessed_Ethanol_US.rename(columns={'settle': 'Ethanol futures contract, USD, closest to spot price for year-month'})\n",
    "# preprocessed_Indian_Rupee_USD = preprocessed_Indian_Rupee_USD.rename(columns={'settle': 'USD-INR FX Future, closest contract to spot price'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2144a699",
   "metadata": {},
   "source": [
    "NOTE: For Oilworld data we are using the lowest representative asking prices for nearest forward shipment in bulk (excl. import duty if any) US-$/T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25b616e",
   "metadata": {},
   "source": [
    "### Combining data\n",
    "Here a number of aggregation transformations are done in order to combine the data across multiple input sources into a master table that will be used for training and runnin the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d92a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "# MASTER_TABLE: merge barchart features into oilworld dataframe on month and year to create master table\n",
    "master_table = Oilworld_data.merge(---INSERT VARIABLE NAME HERE---, on=['month', 'year'], how='left')\n",
    "master_table = master_table.merge(---INSERT VARIABLE NAME HERE---, on=['month', 'year'], how='left')\n",
    "...\n",
    "...\n",
    "master_table = master_table.merge(target_data, on=['month', 'year'], how='left')\n",
    "\n",
    "###Example\n",
    "# master_table = Oilworld_data.merge(preprocessed_Soybean_Oil_US, on=['month', 'year'], how='left')\n",
    "# master_table = master_table.merge(preprocessed_Ethanol_US, on=['month', 'year'], how='left')\n",
    "# master_table = master_table.merge(preprocessed_Indian_Rupee_USD, on=['month', 'year'], how='left')\n",
    "# master_table = master_table.merge(preprocessed_CPO_USD_MDEX_Malaysia, on=['month', 'year'], how='left')\n",
    "# master_table = master_table.merge(target_data, on=['month', 'year'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87a549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table['date'] = pd.to_datetime(master_table[['year', 'month', 'day']])\n",
    "master_table = master_table.set_index('date').drop(columns=['MONTHLY PRICES'])\n",
    "master_table = master_table.reset_index().set_index('month').reset_index().set_index('year').reset_index().set_index('date')\n",
    "master_table = master_table.drop(columns=['day']).sort_values(by='date', ascending=True).loc[master_table.year > 2011]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1e6830",
   "metadata": {},
   "source": [
    "### Handling NaNs and Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e27dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agreed that going forward we will implement mean imputation (forward/back-fill mean) for missing month-years\n",
    "master_table = pd.concat([master_table.ffill(), master_table.bfill()]).groupby(level=0).mean()\n",
    "\n",
    "# Alternative more simple approach for mean imputation\n",
    "# master_table = master_table.fillna(0)\n",
    "# master_table = handle_zero(master_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d715afb",
   "metadata": {},
   "source": [
    "### Transforming Skewed Continuous Features\n",
    "A dataset may sometimes contain at least one feature whose values tend to lie near a single number, but will also have a non-trivial number of vastly larger or smaller values than that single number.  Algorithms can be sensitive to such distributions of values and can underperform if the range is not properly normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b7ae2",
   "metadata": {},
   "source": [
    "For highly-skewed feature distributions, it is common practice to apply a <a href=\"https://en.wikipedia.org/wiki/Data_transformation_(statistics)\">logarithmic transformation</a> on the data so that the very large and very small values do not negatively affect the performance of a learning algorithm. Using a logarithmic transformation significantly reduces the range of values caused by outliers. Care must be taken when applying this transformation however: The logarithm of `0` is undefined, so we must translate the values by a small amount above `0` to apply the the logarithm successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b38339",
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "# Log-transform the skewed features\n",
    "skewed = [\n",
    "    \"---INSERT VARIABLE NAME HERE---\"\n",
    "    ...\n",
    "    ...\n",
    "]\n",
    "\n",
    "features_log_transformed = pd.DataFrame(data = master_table)\n",
    "features_log_transformed[skewed] = master_table[skewed].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "###Example:\n",
    "# skewed = [\n",
    "#     \"Soybean oil,U.S.,fob Decatur\",\n",
    "#     \"Palm stearin RBD, Mal,cif Rott\",\n",
    "#     \"PFAD, Mal fob\",\n",
    "#     \"Palmkern oil,Mal/Indo,cif Rott\",\n",
    "#     \"Soybean oil futures contract, USD, closest to spot price for year-month\",\n",
    "#     \"Ethanol futures contract, USD, closest to spot price for year-month\",\n",
    "#     \"USD-INR FX Future, closest contract to spot price\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a42730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: try further outlier detection/removal here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddf82cb",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74e4f53",
   "metadata": {},
   "source": [
    "### Example strategy from expert meetings related to commodity price forecasting for CPO\n",
    "\n",
    "1. Start with commodity balance sheet items\n",
    "2. Then move on to political drivers, followed by technical indicators, weather, and FX if time allows\n",
    "--- \n",
    "1. ##### Balance Sheet\n",
    "    1. Price spread between palm and Argentine soybean oil – FCPO MDEX vs. Chicago contract\n",
    "    2. Price spread between palm olein (Malaysia OTC) vs. Argentine soybean oil - if less than 100 USD then shift from palm to soybean (close to 50 dollars you can clearly see this shift) , and in the winter months even more (when palm struggles)\n",
    "    3. Draw between CPO and closing stocks. Production is important but closing stock is key for driving price!\n",
    "        1. Big destinations (during last 2 years) all run on very tight stocks – India has run on half the stock that it normally does (all the countries actually, all except China) – reason being price went up is that bank limits stay the same\n",
    "    4. Bulk vs. non-bulk --- helps show demand distribution going out of producing countries (big amount goes to Africa)\n",
    "    5. Correlation between domestic consumption and price band \n",
    "        1. High correlation usually means domestic consumption below 200,000 - 300,000 metric tons\n",
    "        2. COVID did not have an impact on consumption but added a lot of price volatility to supply - timing of COVID coincided with DCE being closed -- went limit down straight away\n",
    "        3. Consumption actually went up in 2021, just a bit down in 2020, not much demand destruction overall\n",
    "    6. FCPO backwardation and contango\n",
    "        1. High backwardation months you would find strong correlation with price movements \n",
    "        2. Alot of interest in palm oil as replacement for gas as a result\n",
    "        3. When backwardation is steep they’ll go buy the backend of the curve and sell the backend on the gas oil - backwardation is influenced by the palm oil buyers\n",
    "        4. Forward curve slope -- some markets take advantage of later months, but India etc. markets don’t take advantage of this (those countries don’t have the ability to\n",
    "\n",
    "2. ##### Political drivers between producing and consuming countries\n",
    "    1. Indonesia biofuel mandate --- most successful in the world (every ton CPO exported has to attract x) -- this goes into fund that is then used to (google for example B30 – check if you can use this)\n",
    "        1. The amount of money the fund has (shows how long the program can last) --- price goes up if the program is going well - duty is the function of the price\n",
    "    3. Malaysia does not  currently have an enforced biofuel mandate\n",
    "    4. Tariffs can be extracted via export and shipping duties:\n",
    "        1. Export duties - fixed base, the average price from 20-19th of every month ) - common tender price established in Indonesia daily\n",
    "        2. You can predict the export duties based on the established daily common tender price ---- CPO, CPKO, etc.\n",
    "        \n",
    "\n",
    "3. ##### Technical indicators:\n",
    "    1. Worth looking into MACD, RSI, and potentially other momentum indicators\n",
    "    1. Indonesian plantations and Chinese companies primarily look at charts because they don’t have strong/rigorous research desk. This might have a knock-on effect because traders might try to push price up or down because of certain support levels (self-fulfilling effect of companies using same indicators)\n",
    "\n",
    "4. ##### Weather\n",
    "    1. Look into precipitation data, especially from Malaysia where more data is available\n",
    "    2. All the roads etc get blocked in large producing countries when rain is too heavy, so more difficult to extract palm fruit and oil extract rates go down\n",
    "    3. Annual filings of Wilmar, Cargill, FGB, etc. for oil extraction\n",
    "\n",
    "5. ##### FX\n",
    "    1. Not so important (not much volatility, including during COVID)\n",
    "----------\n",
    "#### Data sources:\n",
    "1. Barchart, Oilword, Reuters\n",
    "2. MPOB – Malaysian Palm Oil Board --- they have a statistics column frommwhich we could create the entire balance sheet\n",
    "3. SCS surveyor information from Malaysia\n",
    "4. Bal data for weather and fertilizer data related to palm – Oilworld might have some as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbeae45",
   "metadata": {},
   "source": [
    "#### Dropping all unnecessary and heavily correllated target-related columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a1a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "# Drop unecessary columns\n",
    "features_raw = features_log_transformed.drop(columns=['year', 'month'])\n",
    "\n",
    "# Select features to drop (some need to be renamed, dropping temporarily)\n",
    "features_raw = features_raw.drop(columns=[\n",
    "    \"---INSERT VARIABLE NAME HERE---\",\n",
    "    ...\n",
    "    ...\n",
    "], axis = 1)\n",
    "\n",
    "###Example:\n",
    "# features_raw = features_raw.drop(columns=[\n",
    "#     'Crude palm oil futures contract, USD (MDEX), closest to spot price for year-month'\n",
    "#     \"Palm oil crude, cif Rotterdam\"\n",
    "# ], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f74e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any duplicates and reset index to run automated feature generation\n",
    "features_raw = features_raw.reset_index().drop_duplicates(subset=['date'], keep='first').set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75a464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "# Save targets to append later\n",
    "targets = features_raw.loc[\"---INSERT TARGET NAME HERE---\", ..., ...]\n",
    "\n",
    "###Example:\n",
    "# targets = features_raw.loc[\"%_diff_price_current_vs_Median_1_3\", \"%_diff_price_current_vs_Median_4_6\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bac9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safeguard: handle any remaining NaNs w/ forward/backfill mean imputation\n",
    "features_raw = pd.concat([features_raw.ffill(), features_raw.bfill()]).groupby(level=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db31fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "# Drop any remaining featrues that may create target leak\n",
    "features_raw = features_raw.drop(columns=[\"---INSERT TARGET NAME HERE---\", ..., ...])\n",
    "features_raw = features_raw.reset_index()\n",
    "features_raw = features_raw.set_index(pd.DatetimeIndex(features_raw['date']))\n",
    "\n",
    "###Example:\n",
    "# features_raw = features_raw.drop(columns=[\"Palm oil RBD, Mal, fob\", \"Palm olein RBD, Mal, fob\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdb5a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build input list for the lags to be created with feature auto-generation\n",
    "x_list_input = createList(6)\n",
    "x_list_input = x_list_input[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd2653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_raw = calculate_features(features_raw, features=features_raw.columns[1:27], x_list=x_list_input, index_col='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e5df5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-combine auto-generated features w/ target\n",
    "features_raw = features_raw.join(targets).drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db575e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safeguard: Run forward/backfill mean imputation as a safeguard to account for inf in the data created during automated feature engineering\n",
    "features_raw = features_raw.replace([np.inf, -np.inf], np.nan)\n",
    "features_raw = pd.concat([features_raw.ffill(), features_raw.bfill()]).groupby(level=0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d710945f",
   "metadata": {},
   "source": [
    "### Normalizing Numerical Features\n",
    "In addition to performing transformations on features that are highly skewed, it is often good practice to perform some type of scaling on numerical features. Applying a scaling to the data does not change the shape of each feature's distribution; however, normalization ensures that each feature is treated equally when applying supervised learners. Note that once scaling is applied, observing the data in its raw form will no longer have the same original meaning, as exampled below.\n",
    "\n",
    "We will use [`sklearn.preprocessing.MinMaxScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8332aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "# Import sklearn.preprocessing.StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize a scaler, then apply it to the features\n",
    "scaler = MinMaxScaler() # default=(0, 1)\n",
    "\n",
    "numerical = [\n",
    "    \"---INSERT VARIABLE NAME HERE---\",\n",
    "    ...\n",
    "    ...\n",
    "]\n",
    "\n",
    "###Example: \n",
    "# numerical = [\n",
    "#     'Crude palm oil futures contract, USD (MDEX), closest to spot price for year-month',\n",
    "#     \"Palm oil crude, cif Rotterdam\"\n",
    "# ]\n",
    "\n",
    "### Example for fast implementation - Make sure to only run on input feature columns (easiest to use iloc with big datasets such as example below)\n",
    "# numerical = features_raw.iloc[:,:2470].columns\n",
    "\n",
    "features_log_minmax_transform = pd.DataFrame(data = features_raw)\n",
    "features_log_minmax_transform[numerical] = scaler.fit_transform(features_raw[numerical])\n",
    "\n",
    "# Show an example of a record with scaling applied\n",
    "display(features_log_minmax_transform.head(n = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af11e3e",
   "metadata": {},
   "source": [
    "# Automated Modeling Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f19be4",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ba5cda",
   "metadata": {},
   "source": [
    "The following supervised regressor algorithms have been chosen for the initial model:\n",
    "\n",
    "**Decision Tree**\n",
    "- Real World Application:\n",
    "   - Decision Trees are often applied to customer relationship management systems (CRMs). For example, they can be used to investigate the relationships between the customers’ needs and preferences and the success of online shopping (Lee et al. 2007).\n",
    "- Strengths: \n",
    "   - They easily handle feature interactions (one-up over Naive Bayes) and they’re non-parametric, so you don’t have to worry about outliers or whether the data is linearly separable. They’re also fast and scalable, and you don’t have to worry about tuning a bunch of parameters like you do with other classifiers like SVMs. In this case, if the data includes feature interactions that are not linearly-separable, then a Decision Tree will still perform well.\n",
    "- Weaknessses:\n",
    "   - One disadvantage is that they don’t support online learning, so you have to rebuild your tree when new examples come on. Another disadvantage is that they easily overfit, but that’s where ensemble methods like random forests (or boosted trees) come in. In this case overfitting may occur, which is why the next algorithm I chose incorporates an ensemble method.\n",
    "- What makes this model a good candidate for the problem, given what you know about the data?\n",
    "   - Since Decision Trees can easily handle feature interactions without sacrificing much in speed or cost they seem like a good alternative to Naive Bayes in the event that feature interactions are present in the data.\n",
    "   \n",
    "   \n",
    "**Random Forest**\n",
    "- Real World Application:\n",
    "   - Random Forest is used in numerous applications for regression and classification when predicting on large well-established datasets (for example in manufacturing or agriculture where large amounts of data are often collected using sensors)\n",
    "- Strengths: \n",
    "   - Random Forest is suitable for situations when we have a large dataset, and interpretability is not a major concern\n",
    "- Weaknessses:\n",
    "   - Since a Random Forest combines multiple decision trees, it becomes more difficult to interpret, however one can take the best trees to build micro-segmentation if required. Decision trees are much easier to interpret and understand. \n",
    "- What makes this model a good candidate for the problem, given what you know about the data?\n",
    "   - Since Random Forest can easily handle large feature sets without sacrificing much in speed or cost they seem like a good alternative to Decision Tree when our feature space is large.\n",
    "   \n",
    "   \n",
    "**Gradient-Boosted Trees**\n",
    "- Real World Application:\n",
    "   - Gradient-boosted tree algorithms (eg., XGBoost, LightGBM) are applied to all kinds of use cases where model explainability is less important than accuracy. This is because feature relevance will be harder to interpret and not to scale (for example you will rely on SHAP scores that cannot be used to inform micro-segmentation and rules-based marketing if desired).\n",
    "- Strengths: \n",
    "   - Gradient-boosted trees generally provide higher accuracy models when compared to other tree-based/ensemble approaches\n",
    "- Weaknessses:\n",
    "   - Model interpretability becomes more challenging and the model may take longer to train depending on parameters being used\n",
    "- What makes this model a good candidate for the problem, given what you know about the data?\n",
    "   - Can be a good way to demonstrate that even better models are possible if accuracy is poor with other tree-based/ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332d415f",
   "metadata": {},
   "source": [
    "### Obtain final features table (includes pre-processed input features as well as all target features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1ae634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain final features from pre-processing\n",
    "features_final = features_log_minmax_transform\n",
    "\n",
    "#Safeguard: Forward/Back-fill mean imputation for any missing values (safeguard for Null/Inf values created post-log-minmax transformations)\n",
    "features_final = pd.concat([features_final.ffill(), features_final.bfill()]).groupby(level=0).mean()\n",
    "\n",
    "# Alternative more simple approach for mean imputation\n",
    "# master_table = master_table.fillna(0)\n",
    "# master_table = handle_zero(master_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c3c762",
   "metadata": {},
   "source": [
    "### Feature Transformation (Optional)\n",
    "#### Implementation: PCA  (Dimensionality Reduction)\n",
    "Now that the data has been scaled to a more normal distribution and has had any necessary outliers removed, I decided to apply PCA to `features_final` to discover which dimensions about the data best maximize the variance of features involved. In addition to finding these dimensions, PCA will also report the explained variance ratio of each dimension — how much variance within the data is explained by that dimension alone. Note that a component (dimension) from PCA can be considered a new \"feature\" of the space, however it is a composition of the original features present in the data.\n",
    "\n",
    "When using principal component analysis, one of the main goals is to reduce the dimensionality of the data — in effect, reducing the complexity of the problem. Dimensionality reduction comes at a cost: Fewer dimensions used implies less of the total variance in the data is being explained. Because of this, the cumulative explained variance ratio is extremely important for knowing how many dimensions are necessary for the problem. Additionally, if a signifiant amount of variance is explained by only two or three dimensions, the reduced data can be visualized afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e19de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "***(OPTIONAL)\n",
    "# Select targets to drop when running PCA\n",
    "drop_targets = [\"---INSERT TARGETS TO DROP HERE---\", ..., ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67087bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "***(OPTIONAL)\n",
    "# Apply PCA by fitting the good data with only two dimensions\n",
    "pca = PCA(n_components= \"---INSERT NUMBER OF PRINCIPAL COMPONENTS TO USE HERE (AS INTEGER)---\")\n",
    "\n",
    "###Example:\n",
    "# pca = PCA(n_components= 7)\n",
    "\n",
    "pca.fit(features_final.drop(columns=drop_targets))\n",
    "\n",
    "# Transform the good data using the PCA fit above\n",
    "reduced_data = pca.transform(features_final.drop(columns = drop_targets))\n",
    "\n",
    "# Create a DataFrame for the reduced data\n",
    "reduced_data = pd.DataFrame(reduced_data, columns = [\"---INSERT DIMENSIONS HERE---\", ..., ...])\n",
    "\n",
    "###Example:\n",
    "# reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2', 'Dimension 3', 'Dimension 4', 'Dimension 5', 'Dimension 6', 'Dimension 7'])\n",
    "\n",
    "reduced_data.index = features_final.index\n",
    "\n",
    "# Re-combine targets and display data after applying PCA transformation in 7 dimensions\n",
    "reduced_data = pd.merge(reduced_data, features_final[targets], left_index=True, right_index=True, how='left')\n",
    "reduced_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec0cd19",
   "metadata": {},
   "source": [
    "### Model evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb630a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used for quick and dirty gridsearch across multiple algorithms\n",
    "def regression_results(y_true, y_pred):\n",
    "    # Regression metrics\n",
    "#     explained_variance=metrics.explained_variance_score(y_true, y_pred)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mean_absolute_pct_error=metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "#     mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "#     median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "#     r2=metrics.r2_score(y_true, y_pred)\n",
    "#     print('explained_variance: ', round(explained_variance,4))    \n",
    "#     print('r2: ', round(r2,4))\n",
    "    print('MAE: ', round(mean_absolute_error,4))\n",
    "    print('MAPE: ', round(mean_absolute_pct_error,4))\n",
    "#     print('MSE: ', round(mse,4))\n",
    "#     print('RMSE: ', round(np.sqrt(mse),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68603b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for scoring with gridsearchCV\n",
    "def performance_metric(y_true, y_predict):\n",
    "    \"\"\" Calculates and returns the performance score between \n",
    "        true and predicted values based on the metric chosen. \"\"\"\n",
    "    score = mean_squared_error(y_true, y_predict, squared=False)\n",
    "    # Return the score\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4552429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for scoring with gridsearchCV\n",
    "def rmse(actual, predict):\n",
    "    predict = np.array(predict)\n",
    "    actual = np.array(actual)\n",
    "    distance = predict - actual\n",
    "    square_distance = distance ** 2\n",
    "    mean_square_distance = square_distance.mean()\n",
    "    score = np.sqrt(mean_square_distance)\n",
    "    return score\n",
    "\n",
    "rmse_score = make_scorer(rmse, greater_is_better = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aa0452",
   "metadata": {},
   "source": [
    "### Pre-modelling feature selection\n",
    "Correlation analysis \n",
    "F-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3403aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F-test feature selection - implemented within the modeling pipeline below\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    # configure to select a subset of features\n",
    "    fs = SelectKBest(score_func=f_regression, k=5)\n",
    "    # learn relationship from training data\n",
    "    fs.fit(X_train, y_train)\n",
    "    # transform train input data\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    # transform test input data\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    \n",
    "    return X_train_fs, X_test_fs, fs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539cead7",
   "metadata": {},
   "source": [
    "Boruta feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff700d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boruta feature selection - implemented within the modeling pipeline below\n",
    "def select_features_boruta(model, X_train, y_train, X_test):\n",
    "    boruta = BorutaPy(\n",
    "       estimator = model, \n",
    "       n_estimators = 'auto',\n",
    "       max_iter = 100 # number of trials to perform\n",
    "    )\n",
    "    ### fit Boruta (it accepts np.array, not pd.DataFrame)\n",
    "    boruta.fit(np.array(X_train), np.array(y_train))\n",
    "    \n",
    "    # transform train input data\n",
    "    X_train_fs = boruta.transform(X_train)\n",
    "    \n",
    "    # transform test input data\n",
    "    X_test_fs = boruta.transform(X_test)\n",
    "\n",
    "    ### print results\n",
    "    green_area = X_train.columns[boruta.support_].to_list()\n",
    "    blue_area = X_train.columns[boruta.support_weak_].to_list()\n",
    "    print('features in the green area:', green_area)\n",
    "    print('features in the blue area:', blue_area)\n",
    "    \n",
    "    return X_train_fs, X_test_fs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbadd5d1",
   "metadata": {},
   "source": [
    "### Train and evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7daa3c",
   "metadata": {},
   "source": [
    "Below function implements TimeSeriesSplit to perform expanding window cross-validation on the training set, then evaluates across multiple metrics on the out of sample test set. This function can be chained to make step-predictions which different specified training_size, n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea3814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "def run_price_forecasting(select_target:str, features_final:pd.DataFrame, feature_importances_table:pd.DataFrame, train_size:int, overlap_drop:int, num_splits:int, confidence:int):\n",
    "\n",
    "    #create list of targets not in use to be dropped before doing time series split. Conversion to ML supervised learning training set will then be done on training data sans target\n",
    "    drop_features = [\"---INSERT HERE TARGET FEATURES NOT BEING USED THAT WILL BE DROPPED---\"]\n",
    "    \n",
    "    #Example:\n",
    "    # drop_features = ['%_diff_price_current_vs_Median_1', '%_diff_price_current_vs_Median_2', '%_diff_price_current_vs_Median_3']\n",
    "    \n",
    "    index = drop_features.index(select_target)\n",
    "    del drop_features[index]    #removes the selected target from the drop features list\n",
    "    features_final = features_final.drop(columns=drop_features)\n",
    "    features_final = features_final.reset_index().drop_duplicates(subset=['date'], keep='first').set_index('date')\n",
    "\n",
    "    # Create training set\n",
    "    df_train = features_final[:train_size]\n",
    "\n",
    "    # Create test set\n",
    "    df_test = features_final[train_size:] \n",
    "\n",
    "    for col in df_train, df_test:\n",
    "        plt.title('Commodity price forecasting train and test sets', size=20)\n",
    "        plt.plot(df_train, label='Training set')\n",
    "        plt.plot(df_test, label='Test set', color='orange')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    #Create X_train, X_test, y_train, y_test\n",
    "    X_train = df_train.drop(columns=[select_target])\n",
    "    y_train = df_train[select_target]\n",
    "    X_test = df_test.drop(columns=[select_target])\n",
    "    y_test = df_test[select_target]\n",
    "\n",
    "    # Removing last n rows to prevent target leak with training set (targets are forward looking)\n",
    "    X_train = X_train.iloc[:-overlap_drop]\n",
    "    y_train = y_train.iloc[:-overlap_drop]\n",
    "\n",
    "    # Insert model to be used\n",
    "    model = \"---INSERT MODEL BEING USED HERE---\" \n",
    "    #Example: model = RandomForestRegressor(bootstrap=True)\n",
    "\n",
    "    # Insert params grid to be used during gridsearch\n",
    "    params_search = {\"---INSERT PARAMS GRID HERE---\"}\n",
    "    \n",
    "    #Example:\n",
    "    #     param_search = { \n",
    "    #         'n_estimators': [10, 20, 50],\n",
    "    #         'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    #         'max_depth' : [i for i in range(5,15)]\n",
    "    #     }\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"...........Now selecting features\")\n",
    "    \n",
    "    # Feature selection: Boruta, revert to F-test if empty green/blue area\n",
    "    boruta = BorutaPy(\n",
    "    estimator = model, \n",
    "    n_estimators = 'auto',\n",
    "    max_iter = 100) # number of trials to perform\n",
    "    \n",
    "    boruta.fit(np.array(X_train), np.array(y_train))\n",
    "    green_area = X_train.columns[boruta.support_].to_list()\n",
    "    blue_area = X_train.columns[boruta.support_weak_].to_list()\n",
    "    print('features in the green area:', green_area)\n",
    "    print('features in the blue area:', blue_area)\n",
    "\n",
    "    if(len(blue_area) != 0):   \n",
    "        X_train_fs = X_train[X_train.columns.intersection(blue_area)]\n",
    "        X_test_fs = X_test[X_test.columns.intersection(blue_area)]\n",
    "    elif((len(blue_area) == 0) and (len(green_area) != 0)):\n",
    "        X_train_fs = X_train[X_train.columns.intersection(green_area)]\n",
    "        X_test_fs = X_test[X_test.columns.intersection(green_area)]\n",
    "    else:\n",
    "        # Feature selection: F-test\n",
    "        X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)  \n",
    "\n",
    "    print(\"Currently selecting following number of features:\")\n",
    "    print(pd.DataFrame(X_train_fs).shape[1])\n",
    "\n",
    "    # Set the gap to be used when performing cross_validation with TimeSeriesSplit from sklearn\n",
    "    gapper = overlap_drop\n",
    "    \n",
    "    # Initialize TimeSeriesSplit for time series cross-validation when performing gridsearch across params (using rmse below due to negative values in target)\n",
    "    tscv = TimeSeriesSplit(n_splits=num_splits, gap=gapper)\n",
    "    gsearch = GridSearchCV(estimator=model, cv=tscv, param_grid=param_search, scoring = rmse_score)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"...........Now training the model\")\n",
    "    gsearch.fit(X_train_fs, y_train)\n",
    "    best_score = gsearch.best_score_\n",
    "    best_model = gsearch.best_estimator_\n",
    "    y_true = y_test.values\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"...........Now predicting results\")\n",
    "    y_pred = best_model.predict(X_test_fs)\n",
    "    regression_results(y_true, y_pred)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"...........Now extracting feature importances\")\n",
    "    \n",
    "    # Extract the feature importances using .feature_importances_ \n",
    "    importances = best_model.feature_importances_\n",
    "    \n",
    "    # Get columns to keep and create new dataframe with those only\n",
    "    if(len(blue_area) != 0):   \n",
    "        for feature in blue_area:\n",
    "            feature = str(feature)\n",
    "        cols = blue_area\n",
    "        features_df_new = X_train.loc[:,cols]\n",
    "    elif((len(blue_area) == 0) and (len(green_area) != 0)):\n",
    "        for feature in green_area:\n",
    "            feature = str(feature)\n",
    "        cols = green_area\n",
    "        features_df_new = X_train.loc[:,cols]\n",
    "    else:\n",
    "        cols = fs.get_support(indices=True)\n",
    "        features_df_new = X_train.iloc[:,cols]\n",
    "    \n",
    "    # Save feature importances w/ Gini score in dictionary\n",
    "    feats = {} # a dict to hold feature_name: feature_importance\n",
    "    for feature, importance in zip(features_df_new.columns, best_model.feature_importances_):\n",
    "        feats[feature] = importance #add the name/value pair \n",
    "    importance_table = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n",
    "    \n",
    "    #Plot Gini importances\n",
    "    importance_table.sort_values(by='Gini-importance').plot(kind='bar')\n",
    "    plt.title(\"Ranking of Gini-importance for selected features\")\n",
    "    \n",
    "    # Append feature importances to master output list\n",
    "    feature_importances_table = feature_importances_table.append(importance_table)\n",
    "    \n",
    "    # Plot MDI\n",
    "    std = np.std([tree.feature_importances_ for tree in best_model.estimators_], axis=0)\n",
    "    forest_importances = pd.Series(importances, index=features_df_new.columns)\n",
    "    fig, ax = plt.subplots()\n",
    "    forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "    ax.set_title(\"Feature importances using MDI\")\n",
    "    ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"...........Now calculating error\")\n",
    "\n",
    "    # Calculate the variance\n",
    "    variance = fci.random_forest_error(best_model, X_train_fs, X_test_fs)\n",
    "\n",
    "    # Plot error bars for predicted target using unbiased variance\n",
    "#     plt.errorbar(y_test, y_pred, yerr=np.sqrt(variance), fmt='o')\n",
    "#     plt.xlabel('Reported')\n",
    "#     plt.ylabel('Predicted')\n",
    "#     plt.show()\n",
    "\n",
    "    # Prepare new dataframe to store confidence intrervals\n",
    "    confidence_intervals = pd.DataFrame(variance)\n",
    "    confidence_intervals = confidence_intervals.rename(columns={confidence_intervals.columns[0]: 'variance'})\n",
    "    confidence_intervals[\"predictions\"] = y_pred.tolist()\n",
    "    \n",
    "    #Set Z Scores for normal dist. based on confidence\n",
    "    zscore = 1.65 #default \n",
    "    if(confidence == 90):\n",
    "        zscore = 1.65\n",
    "    elif(select_target == 95):\n",
    "        zscore = 1.96\n",
    "    elif(select_target == 99):\n",
    "        zscore = 2.58\n",
    "\n",
    "    #DEFINE CI REQUIREMENT\n",
    "    confidence_intervals['std'] = np.sqrt(confidence_intervals['variance'])\n",
    "    confidence_intervals['upper_bound'] = confidence_intervals['predictions'] + (confidence_intervals['std'] * zscore)\n",
    "    confidence_intervals['lower_bound'] = confidence_intervals['predictions'] - (confidence_intervals['std'] * zscore)\n",
    "\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"...........Now appending results to final df\")\n",
    "    \n",
    "    # Create empty dataframe for forecast results to be exported to excel format (for each select target for which the model is run on a given training window this will be updated)\n",
    "    Forecast_results = pd.DataFrame(X_test_fs)\n",
    "\n",
    "    #Append predictions\n",
    "    col_name1 = 'prediction'\n",
    "    Forecast_results[col_name1] = y_pred.tolist()\n",
    "    \n",
    "    #Append actuals\n",
    "    col_name2 = 'actual'\n",
    "    Forecast_results[col_name2] = y_test.tolist()\n",
    "\n",
    "    #Append upper bound\n",
    "    col_name3 = 'upper_bound'\n",
    "    Forecast_results[col_name3] = confidence_intervals['upper_bound'].tolist()\n",
    "\n",
    "    #Append lower bound\n",
    "    col_name4 = 'lower_bound'\n",
    "    Forecast_results[col_name4] = confidence_intervals['lower_bound'].tolist()\n",
    "\n",
    "    #Append number of forecast months\n",
    "    col_name5 = 'number_of_forecast_month'\n",
    "    Forecast_results[col_name5] = gapper\n",
    "    \n",
    "    #Reduce forecast results for this particular target using this particular training window to just the prediction for first month on out of sample\n",
    "    Forecast_results = pd.DataFrame(Forecast_results[[col_name1, col_name2, col_name3, col_name4, col_name5]])\n",
    "    Forecast_results = Forecast_results.iloc[0, :]\n",
    "    \n",
    "    return Forecast_results, feature_importances_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d06057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "# SET TRAINING WINDOW\n",
    "training_window = \"---INSERT TRAINING WINDOW HERE---\" #take first x months as training set\n",
    "###Example: training_window = 48\n",
    "\n",
    "# DATE TO FORECAST\n",
    "forecast_date_index = features_final.index\n",
    "\n",
    "# SET FORECAST LENGTH\n",
    "forecast_length = len(forecast_date_index) - training_window\n",
    "\n",
    "# SET EMPTY SERIES TO APPEND FEATURE IMPORTANCES\n",
    "complete_feature_importances = pd.DataFrame()\n",
    "master_feature_list = pd.DataFrame()\n",
    "\n",
    "###############RUN THE PREDICTIONS\n",
    "# Create empty dataframe to store all final results for model predicting next 1-3 months median as well as next 4-6\n",
    "Final_forecast_results = pd.DataFrame()\n",
    "\n",
    "targets = [{'target': \"---INSERT TARGET HERE---\", 'offset': \"---INSERT OFFSET HERE---\"}, ..., ...]\n",
    "###Example: targets = [{'target':'%_diff_price_current_vs_Median_1_3', 'offset':3},{'target':'%_diff_price_current_vs_Median_4_6', 'offset':6}]\n",
    "\n",
    "\n",
    "for i in range(forecast_length):\n",
    "    #TODO: initiate empty dataframe here and for every day append with x=1\n",
    "    daily_forecast_result = pd.DataFrame()\n",
    "    decision_month = forecast_date_index[training_window] \n",
    "    current_month = 0\n",
    "    for target in targets:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            #BELOW LINE RUNS THE MODEL - In order to use PCA meta-feature set just replace 'features_final' in the function call with 'reduced_data'\n",
    "            globals()['Forecast_Results_%s' % target['target'].replace(\"%\",\"\")], complete_feature_importances = run_price_forecasting(target['target'], features_final, feature_importances_table=complete_feature_importances, train_size=training_window, overlap_drop=target['offset'], num_splits=3, confidence=95) \n",
    "            master_feature_list = master_feature_list.append(complete_feature_importances)\n",
    "            \n",
    "            for month_iter in range(current_month, target['offset']):\n",
    "                target_month = month_iter + 1\n",
    "                # prediction  upper_bound  lower_bound\n",
    "                copy_of_forecast = pd.DataFrame(copy.deepcopy(globals()['Forecast_Results_%s' % target['target'].replace(\"%\",\"\")])).T[['prediction', 'actual', 'upper_bound', 'lower_bound']]\n",
    "                copy_of_forecast = copy_of_forecast.rename(columns = {\n",
    "                    \"prediction\":\"forecast_result_{}\".format(target_month), \n",
    "                    \"actual\":\"actual_{}\".format(target_month), \n",
    "                    \"upper_bound\":\"forecast_upper_{}\".format(target_month), \n",
    "                    \"lower_bound\":\"forecast_lower_{}\".format(target_month),\n",
    "                    })\n",
    "                daily_forecast_result = pd.concat([daily_forecast_result, copy_of_forecast], axis=1)\n",
    "\n",
    "            current_month = target['offset']\n",
    "    daily_forecast_result['number_of_forecast_month'] = current_month \n",
    "    daily_forecast_result['Date'] = decision_month \n",
    "    Final_forecast_results = Final_forecast_results.append(daily_forecast_result)\n",
    "    training_window += 1\n",
    "Final_forecast_results = Final_forecast_results.set_index(['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf28f636",
   "metadata": {},
   "source": [
    "### Feature importances\n",
    "Plot key features across all back-tested model runs (grouped by mean Gini-importance - consider removing features that showed less than x times across all runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8a6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain feature importance value counts in order to take only those features which have shown more than x (currently 10) times\n",
    "complete_feature_importances['value_count'] = complete_feature_importances.index.value_counts()\n",
    "complete_feature_importances = complete_feature_importances[complete_feature_importances.value_count >= 10]\n",
    "complete_feature_importances = complete_feature_importances.sort_values(by='Gini-importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dc822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset the features grouped by the mean Gini-importance score of their ocurrence across backtesting model runs\n",
    "key_drivers = complete_feature_importances.reset_index()\n",
    "key_drivers = key_drivers.drop(columns=['value_count'])\n",
    "key_drivers = key_drivers.groupby('index').mean()\n",
    "key_drivers = key_drivers.sort_values(by='Gini-importance', ascending=False)\n",
    "\n",
    "# Plot features ranksed by Gini-importance for backtested model\n",
    "key_drivers = key_drivers.sort_values(by='Gini-importance', ascending=False)\n",
    "key_drivers.plot(kind='bar')\n",
    "plt.title(\"Ranking of Gini-importance for selected features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1138e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to excel\n",
    "key_drivers.to_excel(r'Key_Drivers.xlsx', index = True)\n",
    "key_drivers.to_csv(r'Key_Drivers.csv', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed2b9c",
   "metadata": {},
   "source": [
    "## Final results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94ded5d",
   "metadata": {},
   "source": [
    "#### Obtain actuals and predictions columns from the desired forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09871c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_forecast_results = Final_forecast_results[~Final_forecast_results.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a69790",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET COLUMNS TO RUN FINAL PLOTS AND EVALUATION\n",
    "y_test = Final_forecast_results['actual_2']\n",
    "y_pred = Final_forecast_results['forecast_result_2']\n",
    "upper = Final_forecast_results['forecast_upper_2']\n",
    "lower = Final_forecast_results['forecast_lower_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089e02fd",
   "metadata": {},
   "source": [
    "#### Plot results vs. predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a1f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results vs. predicted for each forecast\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('Reported')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b73586",
   "metadata": {},
   "source": [
    "#### MAE on final forecast results, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaabd05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot each of the forecast vs actual on line chart and then run the MAE for each of the forecasts \n",
    "x = y_test.index\n",
    "plt.plot(x, upper, color = 'yellow')\n",
    "plt.plot(x, lower, color = 'yellow')\n",
    "plt.fill_between(x,lower,upper,interpolate=True,color='yellow')\n",
    "plt.plot(x, y_pred, color = 'purple')\n",
    "plt.plot(x, y_test, color = 'green')\n",
    "plt.show()\n",
    "\n",
    "regression_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300f58d",
   "metadata": {},
   "source": [
    "#### Binary and Multi-class confusion matrices for directional accuracy, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fa14c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build confusion matrix (binary)\n",
    "cutoff = \"---INSERT CUTOFF LIMIT HERE---\"                            \n",
    "\n",
    "###Example:\n",
    "# cutoff = 0\n",
    "\n",
    "y_pred_classes = np.zeros_like(y_pred)    # initialise a matrix full with zeros\n",
    "y_pred_classes[y_pred > cutoff] = 1       # add a 1 if the cutoff was breached\n",
    "\n",
    "# You have to do the same for the actual values too:\n",
    "y_test_classes = np.zeros_like(y_pred)\n",
    "y_test_classes[y_test > cutoff] = 1\n",
    "\n",
    "# Now run the confusion matrix as before:\n",
    "confusion = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec968db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build confusion matrix (multi-class)\n",
    "# cutoff_1 = \"---INSERT CUTOFF LIMIT HERE---\"\n",
    "# cutoff_2 = \"---INSERT CUTOFF LIMIT HERE---\"\n",
    "\n",
    "####Example:\n",
    "# cutoff_1 = -0.05                             # decide on a cutoff limit\n",
    "# cutoff_2 = 0.05\n",
    "\n",
    "# y_pred_classes = np.zeros_like(y_pred)    # initialise a matrix full with zeros\n",
    "# y_pred_classes[y_pred < cutoff_1] = 1       # add a 1 if the cutoff was breached\n",
    "# y_pred_classes[(y_pred >= cutoff_1) & (y_pred < cutoff_2)] = 2       # add a 2 if the cutoff was breached\n",
    "# y_pred_classes[y_pred >= cutoff_2] = 3       # add a 3 if the cutoff was breached\n",
    "\n",
    "\n",
    "# # you have to do the same for the actual values too:\n",
    "# y_test_classes = np.zeros_like(y_test)    # initialise a matrix full with zeros\n",
    "# y_test_classes[y_test < cutoff_1] = 1       # add a 1 if the cutoff was breached\n",
    "# y_test_classes[(y_test >= cutoff_1) & (y_test < cutoff_2)] = 2       # add a 2 if the cutoff was breached\n",
    "# y_test_classes[y_test >= cutoff_2] = 3       # add a 3 if the cutoff was breached\n",
    "\n",
    "\n",
    "# # Now run the confusion matrix as before:\n",
    "# # confusion = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "# # print('Confusion Matrix\\n')\n",
    "# # print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd0a50c",
   "metadata": {},
   "source": [
    "#### Accuracy, precision, recall, F1 using classes built for confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25edd154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_classes, y_pred_classes)))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_classes, y_pred_classes, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_classes, y_pred_classes, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_classes, y_pred_classes, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_test_classes, y_pred_classes, target_names=['Class 1', 'Class 2'])) #Class 3 if you want to use multi-class confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e757e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test_classes, y_pred_classes)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# method I: plt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6682a9f5",
   "metadata": {},
   "source": [
    "#### Conversion of target from % target to actual price for MAPE evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9b48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull required columns from final forecast table\n",
    "price_conversion = Final_forecast_results[[\"---INSERT FORECAST RESULT, PREDICTED LOWER BOUND, PREDICTED UPPER BOUND, AND ACTUAL FOR EACH TARGET PREDICTED\"]]\n",
    "\n",
    "###Example: (grabs targets for % difference from current price vs. 1-3m and 4-6m median -- in this case the results for 1,2,3 are all the same and likewise for 4,5,6)\n",
    "# price_conversion = Final_forecast_results[['forecast_result_2', 'forecast_lower_2', 'forecast_upper_2', 'actual_2', 'forecast_result_4', 'forecast_lower_4', 'forecast_upper_4', 'actual_4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5930f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in real the actual current prices to make conversion \n",
    "price_conversion.index = pd.DatetimeIndex(price_conversion.index)\n",
    "price_conversion = pd.merge(price_conversion,target_data_select, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89f126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert % change targets to prices\n",
    "price_conversion['predicted_price_1_3'] = price_conversion['settle'] * (1+ price_conversion['forecast_result_2'])\n",
    "price_conversion['lower_bound_1_3'] = price_conversion['settle'] * (1+ price_conversion['forecast_lower_2'])\n",
    "price_conversion['upper_bound_1_3'] = price_conversion['settle'] * (1+ price_conversion['forecast_upper_2'])\n",
    "\n",
    "price_conversion['predicted_price_4_6'] = price_conversion['settle'] * (1+ price_conversion['forecast_result_4'])\n",
    "price_conversion['lower_bound_4_6'] = price_conversion['settle'] * (1+ price_conversion['forecast_lower_4'])\n",
    "price_conversion['upper_bound_4_6'] = price_conversion['settle'] * (1+ price_conversion['forecast_upper_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84477450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actuals and CI for model predicting 1-3\n",
    "x = price_conversion.index\n",
    "y_test = price_conversion['Median_1_3']\n",
    "y_pred = price_conversion['predicted_price_1_3']\n",
    "upper = price_conversion['upper_bound_1_3']\n",
    "lower = price_conversion['lower_bound_1_3']\n",
    "\n",
    "plt.plot(x, upper, color = 'yellow')\n",
    "plt.plot(x, lower, color = 'yellow')\n",
    "plt.fill_between(x,lower,upper,interpolate=True,color='yellow')\n",
    "plt.plot(x, y_pred, color = 'purple')\n",
    "plt.plot(x, y_test, color = 'green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af85993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot actuals and CI for model predicting 4-6\n",
    "x = price_conversion.index\n",
    "y_test = price_conversion['Median_4_6']\n",
    "y_pred = price_conversion['predicted_price_4_6']\n",
    "upper = price_conversion['upper_bound_4_6']\n",
    "lower = price_conversion['lower_bound_4_6']\n",
    "\n",
    "plt.plot(x, upper, color = 'yellow')\n",
    "plt.plot(x, lower, color = 'yellow')\n",
    "plt.fill_between(x,lower,upper,interpolate=True,color='yellow')\n",
    "plt.plot(x, y_pred, color = 'purple')\n",
    "plt.plot(x, y_test, color = 'green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e666a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safeguard: Implement forward/back-fill mean imputation for missing month-years\n",
    "price_conversion = pd.concat([price_conversion.ffill(), price_conversion.bfill()]).groupby(level=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800270c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain final regression results\n",
    "regression_results(price_conversion['---INSERT ACTUAL HERE---'], price_conversion['---INSERT PREDICTED HERE---'])\n",
    "...\n",
    "...\n",
    "\n",
    "###Example:\n",
    "# regression_results(price_conversion['Median_1_3'], price_conversion['predicted_price_1_3'])\n",
    "# regression_results(price_conversion['Median_4_6'], price_conversion['predicted_price_4_6'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e518a4",
   "metadata": {},
   "source": [
    "### Prepare final model results for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9ac352",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_conversion = price_conversion.drop(columns=['month', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945765b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe that renames all columns for % change and actual price to legible names\n",
    "final_results = price_conversion\n",
    "final_results.columns = ['predicted_pct_delta_current_vs_median_1_3', 'predicted_lower_pct_delta_current_vs_median_1_3', 'predicted_upper_pct_delta_current_vs_median_1_3', 'actual_pct_delta_current_vs_median_1_3', 'predicted_pct_delta_current_vs_median_4_6', 'predicted_lower_pct_delta_current_vs_median_4_6', 'predicted_upper_pct_delta_current_vs_median_4_6', 'actual_pct_delta_current_vs_median_4_6', 'actual_price', 'actual_median_price_1_3', 'actual_median_price_4_6', 'predicted_median_price_1_3', \"predicted_lower_median_price_1_3\",'predicted_upper_median_price_1_3', 'predicted_median_price_4_6', \"predicted_lower_median_price_4_6\",'predicted_upper_median_price_4_6']\n",
    "\n",
    "# Take the last rows (the very last will be used to give 1-3 and 4-6 month forecast)\n",
    "final_results_last = final_results.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26823ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage ranges for vizualization\n",
    "def calc_range_probabilities(actual, forecasted_result):\n",
    "    diff = np.std(actual - forecasted_result)\n",
    "    deviation = diff * 1.96\n",
    "    \n",
    "    print(\"Standard Deviation is: \")\n",
    "    print(deviation)\n",
    "    \n",
    "    print(\"Range probability 1 is: \")\n",
    "    print(scipy.stats.norm(forecasted_result.tail(1), diff).cdf(-0.05))\n",
    "    \n",
    "    print(\"Range probability 2 is: \")\n",
    "    print(scipy.stats.norm(forecasted_result.tail(1), diff).cdf(0)-scipy.stats.norm(forecasted_result.tail(1), diff).cdf(-0.05))\n",
    "    \n",
    "    print(\"Range probability 3 is: \")\n",
    "    print(scipy.stats.norm(forecasted_result.tail(1), diff).cdf(0.05)-scipy.stats.norm(forecasted_result.tail(1), diff).cdf(0))\n",
    "    \n",
    "    print(\"Range probability 4 is: \")\n",
    "    print(1-scipy.stats.norm(forecasted_result.tail(1), diff).cdf(0.05))\n",
    "    \n",
    "    range_df = pd.DataFrame()\n",
    "    range_df['range_1'] = scipy.stats.norm(forecasted_result, diff).cdf(-0.05)\n",
    "    range_df['range_2'] = scipy.stats.norm(forecasted_result, diff).cdf(0)-scipy.stats.norm(forecasted_result, diff).cdf(-0.05)\n",
    "    range_df['range_3'] = scipy.stats.norm(forecasted_result, diff).cdf(0.05)-scipy.stats.norm(forecasted_result, diff).cdf(0)\n",
    "    range_df['range_4'] = 1-scipy.stats.norm(forecasted_result, diff).cdf(0.05)\n",
    "    \n",
    "    range_df = range_df.tail(1)\n",
    "    \n",
    "    return range_df\n",
    "\n",
    "# Calculate range probabilities for predictions\n",
    "INSERT_VARIABLE_NAME_HERE = calc_range_probabilities(final_results_last['INSERT_VARIABLE_NAME_HERE'], final_results_last['INSERT_VARIABLE_NAME_HERE'])\n",
    "\n",
    "### Examples:\n",
    "# range_df_1_3 = calc_range_probabilities(final_results_last['actual_pct_delta_current_vs_median_1_3'], final_results_last['predicted_pct_delta_current_vs_median_1_3'])\n",
    "# range_df_4_6 = calc_range_probabilities(final_results_last['actual_pct_delta_current_vs_median_4_6'], final_results_last['predicted_pct_delta_current_vs_median_4_6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa17740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build viz table for 1-3m model\n",
    "final_results_viz_1_3 = final_results_last[['predicted_median_price_1_3', 'predicted_upper_median_price_1_3', 'predicted_lower_median_price_1_3', 'predicted_pct_delta_current_vs_median_1_3']]\n",
    "final_results_viz_1_3 = final_results_viz_1_3.tail(1)\n",
    "final_results_viz_1_3['range_1_prob'] = range_df_1_3['range_1'].values\n",
    "final_results_viz_1_3['range_2_prob'] = range_df_1_3['range_2'].values\n",
    "final_results_viz_1_3['range_3_prob'] = range_df_1_3['range_3'].values\n",
    "final_results_viz_1_3['range_4_prob'] = range_df_1_3['range_4'].values\n",
    "final_results_viz_1_3 = final_results_viz_1_3.append([final_results_viz_1_3]*2,ignore_index=False)\n",
    "final_results_viz_1_3.columns = ['price', 'upper', 'lower', 'pct_change', 'range_1_prob', 'range_2_prob', 'range_3_prob', 'range_4_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185d4bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build viz table for 4-6m model\n",
    "final_results_viz_4_6 = final_results_last[['predicted_median_price_4_6', 'predicted_upper_median_price_4_6', 'predicted_lower_median_price_4_6', 'predicted_pct_delta_current_vs_median_4_6']]\n",
    "final_results_viz_4_6 = final_results_viz_4_6.tail(1)\n",
    "final_results_viz_4_6['range_1_prob'] = range_df_4_6['range_1'].values\n",
    "final_results_viz_4_6['range_2_prob'] = range_df_4_6['range_2'].values\n",
    "final_results_viz_4_6['range_3_prob'] = range_df_4_6['range_3'].values\n",
    "final_results_viz_4_6['range_4_prob'] = range_df_4_6['range_4'].values\n",
    "final_results_viz_4_6 = final_results_viz_4_6.append([final_results_viz_4_6]*2,ignore_index=False)\n",
    "final_results_viz_4_6.columns = ['price', 'upper', 'lower', 'pct_change', 'range_1_prob', 'range_2_prob', 'range_3_prob', 'range_4_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f04576",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_viz = final_results_viz_1_3.append(final_results_viz_4_6)\n",
    "final_results_viz['pct_change'] = final_results_viz['pct_change'] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5364553c",
   "metadata": {},
   "source": [
    "###  Write final model results to CSV and Excel for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331427d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write final visualization output in PowerBI to CSV and Excel\n",
    "final_results.to_excel(r'Forecast_Results_Price.xlsx', index = True)\n",
    "final_results.to_csv(r'Forecast_Results_Price.csv', index = True)\n",
    "\n",
    "# Write full predictions + price conversions to CSV and Excel\n",
    "final_results_viz.to_excel(r'Forecast_Results_PowerBI_Viz.xlsx', index = True)\n",
    "final_results_viz.to_csv(r'Forecast_Results_PowerBI_Viz.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f85ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c8ae0f",
   "metadata": {},
   "source": [
    "### Write final model results to CSV and Excel for hedging optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16f7ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to excel\n",
    "Final_forecast_results.to_excel(r'Forecast Results.xlsx', index = True)\n",
    "Final_forecast_results.to_csv(r'Forecast_Results.csv', index = True)\n",
    "\n",
    "# Show final results table\n",
    "Final_forecast_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a1b863",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "Example: The final model's MAPE on the reduced testing data increases/decreases (going from ...% to ...% MAPE on ...-month forecast and from ...% to ...% MAPE on ...-month forecast). The hit rate (precision) on a binary confusion matrix with ... as cutoff for the ...-month model comes to ...%. If training time was the most important factor one might also consider using a reduced dataset for training, however given that we already optimized for speed it would probably make sense to continue using full data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3675430c",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "The following functions are used to pre-process the data being used in the model, run model training and evaluation, identify feature importances, visualize results, and record those results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd34c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-process specific commodity dataframe to contain only closest contract to spot price per month\n",
    "def find_closest_to_spot(data: pd.DataFrame):\n",
    "    data = data.groupby(['price_date']).first().reset_index(drop=False)[['price_date', 'settle']].set_index(['price_date'])\n",
    "    data = data.reset_index()\n",
    "    data.price_date = pd.to_datetime(data.price_date)\n",
    "    data['price_date'] = data.price_date - pd.offsets.MonthBegin(1)\n",
    "    data = data.set_index('price_date')\n",
    "    data = data.groupby(pd.Grouper(freq='M')).mean()\n",
    "    data['month'] = data.index.month\n",
    "    data['year'] = data.index.year\n",
    "    data = data.reset_index()\n",
    "    \n",
    "#    # Optional code for only considering contracts with certain time till expiration\n",
    "#     data = data.sort_values(by=['price_date', 'expiration'], ascending=True)\n",
    "#     data = data.loc[data.days_to_expiration >= 0]\n",
    "#     data = data.loc[data.days_to_expiration <= 5]\n",
    "#     data = data.loc[data.groupby(['month', 'year']).days_to_expiration.idxmin()].reset_index(drop=True)\n",
    "#     data = data.sort_values(by=['year', 'month'], ascending=True)    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc85429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_commodity_date(data, date):\n",
    "    data = data.loc[data.price_date >= date] \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968363fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_barchart_data(preprocessed_data:pd.DataFrame):\n",
    "    preprocessed_data = preprocessed_data[['price_date', 'settle']]\n",
    "    preprocessed_data['price_date'] = pd.to_datetime(preprocessed_data['price_date'])\n",
    "    preprocessed_data['month'] = pd.DatetimeIndex(preprocessed_data['price_date']).month\n",
    "    preprocessed_data['year'] = pd.DatetimeIndex(preprocessed_data['price_date']).year\n",
    "    preprocessed_data = preprocessed_data.set_index('price_date')\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1de8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic mean imputation strategy for zeros that are actually missing values\n",
    "def handle_zero(df: pd.DataFrame):\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].replace(0,df[col].mean())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f580cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createList(n):\n",
    "    lst = []\n",
    "    for i in range(n+1):\n",
    "        lst.append(i)\n",
    "    return(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40deff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_bband(data, x, z):\n",
    "    \"\"\"Bollinger band.\"\"\"\n",
    "    sma = feat_avg(data, x)\n",
    "    std = feat_std(data, x)\n",
    "    bband_upper = sma + std * z\n",
    "    bband_lower = sma - std * z\n",
    "    return bband_upper, bband_lower\n",
    "def feat_rsi(data, x):\n",
    "    \"\"\"Relative strength index.\"\"\"\n",
    "    rsi = RSI(data, timeperiod=14)\n",
    "    return rsi\n",
    "def feat_macd(data, x):\n",
    "    \"\"\"Moving average convergence divergence.\"\"\"\n",
    "    macd, macdsignal, macdhist = MACD(data, fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    return macd\n",
    "def feat_avg(data, x):\n",
    "    \"\"\"Rolling average.\"\"\"\n",
    "    return data.rolling(x, min_periods=1).mean()\n",
    "def feat_med(data, x):\n",
    "    \"\"\"Rolling median.\"\"\"\n",
    "    return data.rolling(x, min_periods=1).median()\n",
    "def feat_min(data, x):\n",
    "    \"\"\"Rolling min.\"\"\"\n",
    "    return data.rolling(x, min_periods=1).min()\n",
    "def feat_max(data, x):\n",
    "    \"\"\"Rolling max.\"\"\"\n",
    "    return data.rolling(x, min_periods=1).max()\n",
    "def feat_std(data, x):\n",
    "    \"\"\"Rolling standard deviation.\"\"\"\n",
    "    return data.rolling(x, min_periods=1).std()\n",
    "def feat_lag(data, x):\n",
    "    \"\"\"Lag terms of current value (x days before).\"\"\"\n",
    "    return data.shift(x)\n",
    "def feat_trend(data, x):\n",
    "    \"\"\"Trend defined as return of x days over mean of the same period.\"\"\"\n",
    "    return (data - data.shift(x)) / feat_avg(data, x)\n",
    "def feat_first_deriv_avg(data, x):\n",
    "    \"\"\"Rolling avg of 1st derivative.\"\"\"\n",
    "    return data.diff().rolling(x, min_periods=1).mean()\n",
    "def feat_second_deriv_avg(data, x):\n",
    "    \"\"\"Rolling avg of 2nd derivative.\"\"\"\n",
    "    return data.diff().diff().rolling(x, min_periods=1).mean()\n",
    "def feat_first_deriv_max(data, x):\n",
    "    \"\"\"Rolling max of fst derivative.\"\"\"\n",
    "    return data.diff().rolling(x, min_periods=1).max()\n",
    "def feat_second_deriv_max(data, x):\n",
    "    \"\"\"Rolling max of 2nd derivative.\"\"\"\n",
    "    return data.diff().diff().rolling(x, min_periods=1).max()\n",
    "def calc_slope(x):\n",
    "    \"\"\"Linear slope helper function.\"\"\"\n",
    "    slope = np.polyfit(np.arange(len(x), dtype=float), x, 1)[0]\n",
    "    return slope\n",
    "def feat_slope(data, x):\n",
    "    \"\"\"Slope of past x days.\"\"\"\n",
    "    return data.rolling(x).apply(calc_slope)\n",
    "\n",
    "def generate_forward_curve(data: pd.DataFrame, fwd_curve_params: Dict):\n",
    "    \"\"\"Generate forward curve with given future contracts.\n",
    "    Args:\n",
    "        data: master dataframe with forward curve in it\n",
    "        fwd_curve_params: Dictionary contain underlying contract of future contracts\n",
    "    Returns:\n",
    "        data: Dataframe with forwrd slope of underlying commodity\n",
    "    \"\"\"\n",
    "    for product in fwd_curve_params[\"product\"]:\n",
    "        fwd_curve_point = fwd_curve_params[\"forward_curve\"]\n",
    "        fwd_curve_col_name = [product + \"_{}\".format(p) for p in fwd_curve_point]\n",
    "        sub_data = data[fwd_curve_col_name].copy()\n",
    "        # calculate the slope of forward curve using continues forward curve data\n",
    "        data[\"{}_fwd_slope\".format(product)] = sub_data.apply(calc_slope, axis=1)\n",
    "        data[\"{}_fwd_slope\".format(product)] = (\n",
    "            data[\"{}_fwd_slope\".format(product)]\n",
    "            .replace(0, np.nan, inplace=False)\n",
    "            .fillna(method=\"ffill\")\n",
    "        )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c923f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(\n",
    "    data: pd.DataFrame, features: list(), x_list: list(), index_col=\"timestamp\"\n",
    "):\n",
    "    \"\"\"Summary.\n",
    "    Args:\n",
    "        data pd.DataFrame: data that contain all raw data used to generate features\n",
    "        features List: name of raw data to generate features\n",
    "        x_list List: lag terms used to generate features\n",
    "        index_col (str, optional): Index columnDefaults to \"timestamp\".\n",
    "    Returns:\n",
    "        pd.DataFrame: featured master table\n",
    "    \"\"\"\n",
    "    data_to_gen = data\n",
    "    data_output = pd.DataFrame(data_to_gen[index_col])\n",
    "    selected_features = features\n",
    "    test_horizon = x_list\n",
    "    for f in selected_features:\n",
    "        for x in test_horizon:\n",
    "            # calculate moving average (check)\n",
    "            data_output[\"feat__{}__mean_{}_months\".format(f, x)] = feat_avg(\n",
    "                data_to_gen[f], x\n",
    "            )\n",
    "            # calculate value over mv avg (check)\n",
    "            data_output[\"feat__{}__current_over_mean_{}_months\".format(f, x)] = (\n",
    "                data_to_gen[f] - data_output[\"feat__{}__mean_{}_months\".format(f, x)]\n",
    "            )\n",
    "            # calculate std (check)\n",
    "            data_output[\"feat__{}__std_{}_months\".format(f, x)] = feat_std(\n",
    "                data_to_gen[f], x\n",
    "            )\n",
    "            # calculate median (check)\n",
    "            data_output[\"feat__{}__median_{}_months\".format(f, x)] = feat_med(\n",
    "                data_to_gen[f], x\n",
    "            )\n",
    "            # calculate min (check)\n",
    "            data_output[\"feat__{}__min_{}_months\".format(f, x)] = feat_min(\n",
    "                data_to_gen[f], x\n",
    "            )\n",
    "            # calculate max (check)\n",
    "            data_output[\"feat__{}__max_{}_months\".format(f, x)] = feat_max(\n",
    "                data_to_gen[f], x\n",
    "            )\n",
    "            # calculate lag (check)\n",
    "            data_output[\"feat__{}__lag_{}_months\".format(f, x)] = feat_lag(\n",
    "                data_to_gen[f], x\n",
    "            )\n",
    "            # calculate delta (check)\n",
    "            data_output[\"feat__{}__delta_{}_months\".format(f, x)] = (\n",
    "                data_to_gen[f] - data_output[\"feat__{}__lag_{}_months\".format(f, x)]\n",
    "            )\n",
    "            # calculate bband (check)\n",
    "            bband_upper, bband_lower = feat_bband(data_to_gen[f], x, 1.5)\n",
    "            data_output[\"feat__{}__bband_upper_{}_months\".format(f, x)] = bband_upper\n",
    "            data_output[\"feat__{}__bband_lower_{}_months\".format(f, x)] = bband_lower\n",
    "            # calculate trend (check)\n",
    "            data_output[\"feat__{}__trend_{}_months\".format(f, x)] = (\n",
    "                data_output[\"feat__{}__delta_{}_months\".format(f, x)]\n",
    "                / data_output[\"feat__{}__mean_{}_months\".format(f, x)]\n",
    "            )\n",
    "            # calculate RSI (check)\n",
    "            data_output[\n",
    "                \"feat__{}__RSI_{}_months\".format(f, x)\n",
    "            ] = feat_rsi(data_to_gen[f], x)\n",
    "            # calculate MACD (check)\n",
    "            data_output[\n",
    "                \"feat__{}__MACD_{}_months\".format(f, x)\n",
    "            ] = feat_macd(data_to_gen[f], x)\n",
    "            # calculate 1st deriv (check)\n",
    "            data_output[\n",
    "                \"feat__{}__avg_first_derivative_{}_months\".format(f, x)\n",
    "            ] = feat_first_deriv_avg(data_to_gen[f], x)\n",
    "            # calculate 2st deriv (check)\n",
    "            data_output[\n",
    "                \"feat__{}__avg_second_derivative_{}_months\".format(f, x)\n",
    "            ] = feat_second_deriv_avg(data_to_gen[f], x)\n",
    "            # calculate 1st deriv (check)\n",
    "            data_output[\n",
    "                \"feat__{}__max_first_derivative_{}_months\".format(f, x)\n",
    "            ] = feat_first_deriv_max(data_to_gen[f], x)\n",
    "            # calculate 2st deriv (check)\n",
    "            data_output[\n",
    "                \"feat__{}__max_second_derivative_{}_months\".format(f, x)\n",
    "            ] = feat_second_deriv_max(data_to_gen[f], x)\n",
    "            # calculate slope\n",
    "            data_output[\"feat__{}__slope_{}_months\".format(f, x)] = feat_slope(\n",
    "                data_to_gen[f], x\n",
    "            )\n",
    "            # calculate zscore\n",
    "            data_output[\"feat__{}__zscore_{}_months\".format(f, x)] = (\n",
    "                data_to_gen[f] - data_output[\"feat__{}__mean_{}_months\".format(f, x)]\n",
    "            ) / data_output[\"feat__{}__std_{}_months\".format(f, x)]\n",
    "            data_output = data_output.copy()\n",
    "    return data_output.copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
